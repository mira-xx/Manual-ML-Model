{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d8d7fad-bae2-48b5-b8f8-df8c712d6ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Documents\\data\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\Lenovo\\Documents\\data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bfbc275-f699-4129-be30-3e034830cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Documents\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b033bcbb-bab5-413c-88ef-c587cfb63983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_brain_samples= 16540 , seen_brain_features= 561\n",
      "seen_image_samples= 16540 , seen_image_features= 100\n",
      "seen_text_samples= 16540 , seen_text_features= 512\n",
      "seen_label= torch.Size([16540, 1])\n",
      "unseen_brain_samples= 16000 , unseen_brain_features= 561\n",
      "unseen_image_samples= 16000 , unseen_image_features= 100\n",
      "unseen_text_samples= 16000 , unseen_text_features= 512\n",
      "unseen_label= torch.Size([16000, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "data_dir_root = os.path.join('./data', 'ThingsEEG-Text')\n",
    "sbj = 'sub-10'\n",
    "image_model = 'pytorch/cornet_s'\n",
    "text_model = 'CLIPText'\n",
    "roi = '17channels'\n",
    "brain_dir = os.path.join(data_dir_root, 'brain_feature', roi, sbj)\n",
    "image_dir_seen = os.path.join(data_dir_root, 'visual_feature/ThingsTrain', image_model, sbj)\n",
    "image_dir_unseen = os.path.join(data_dir_root, 'visual_feature/ThingsTest', image_model, sbj)\n",
    "text_dir_seen = os.path.join(data_dir_root, 'textual_feature/ThingsTrain/text', text_model, sbj)\n",
    "text_dir_unseen = os.path.join(data_dir_root, 'textual_feature/ThingsTest/text', text_model, sbj)\n",
    "\n",
    "brain_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['data'].astype('double') * 2.0\n",
    "brain_seen = brain_seen[:,:,27:60] # 70ms-400ms\n",
    "brain_seen = np.reshape(brain_seen, (brain_seen.shape[0], -1))\n",
    "image_seen = sio.loadmat(os.path.join(image_dir_seen, 'feat_pca_train.mat'))['data'].astype('double')*50.0\n",
    "text_seen = sio.loadmat(os.path.join(text_dir_seen, 'text_feat_train.mat'))['data'].astype('double')*2.0\n",
    "label_seen = sio.loadmat(os.path.join(brain_dir, 'eeg_train_data_within.mat'))['class_idx'].T.astype('int')\n",
    "image_seen = image_seen[:,0:100]\n",
    "\n",
    "brain_unseen = sio.loadmat(os.path.join(brain_dir, 'eeg_test_data.mat'))['data'].astype('double')*2.0\n",
    "brain_unseen = brain_unseen[:, :, 27:60]\n",
    "brain_unseen = np.reshape(brain_unseen, (brain_unseen.shape[0], -1))\n",
    "image_unseen = sio.loadmat(os.path.join(image_dir_unseen, 'feat_pca_test.mat'))['data'].astype('double')*50.0\n",
    "text_unseen = sio.loadmat(os.path.join(text_dir_unseen, 'text_feat_test.mat'))['data'].astype('double')*2.0\n",
    "label_unseen = sio.loadmat(os.path.join(brain_dir, 'eeg_test_data.mat'))['class_idx'].T.astype('int')\n",
    "image_unseen = image_unseen[:, 0:100]\n",
    "\n",
    "brain_seen = torch.from_numpy(brain_seen)\n",
    "brain_unseen = torch.from_numpy(brain_unseen)\n",
    "image_seen = torch.from_numpy(image_seen)\n",
    "image_unseen = torch.from_numpy(image_unseen)\n",
    "text_seen = torch.from_numpy(text_seen)\n",
    "text_unseen = torch.from_numpy(text_unseen)\n",
    "label_seen = torch.from_numpy(label_seen)\n",
    "label_unseen = torch.from_numpy(label_unseen)\n",
    "\n",
    "print('seen_brain_samples=', brain_seen.shape[0], ', seen_brain_features=', brain_seen.shape[1])\n",
    "print('seen_image_samples=', image_seen.shape[0], ', seen_image_features=', image_seen.shape[1])\n",
    "print('seen_text_samples=', text_seen.shape[0], ', seen_text_features=', text_seen.shape[1])\n",
    "print('seen_label=', label_seen.shape)\n",
    "print('unseen_brain_samples=', brain_unseen.shape[0], ', unseen_brain_features=', brain_unseen.shape[1])\n",
    "print('unseen_image_samples=', image_unseen.shape[0], ', unseen_image_features=', image_unseen.shape[1])\n",
    "print('unseen_text_samples=', text_unseen.shape[0], ', unseen_text_features=', text_unseen.shape[1])\n",
    "print('unseen_label=', label_unseen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5521534f-6e8b-4e60-826e-c68d26ab2c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1],\n",
       "        [  2],\n",
       "        [  3],\n",
       "        ...,\n",
       "        [198],\n",
       "        [199],\n",
       "        [200]], dtype=torch.int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffa11fde-c2f0-4cff-928d-a5ab0514246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in train_brain: 0\n",
      "NaN values in train_image: 0\n",
      "NaN values in train_text: 0\n",
      "NaN values in train_label: 0\n",
      "torch.Size([500, 561])\n",
      "torch.Size([500, 100])\n",
      "torch.Size([500, 512])\n",
      "torch.Size([500, 1])\n",
      "torch.Size([500, 561])\n",
      "torch.Size([500, 100])\n",
      "torch.Size([500, 512])\n",
      "torch.Size([500, 1])\n"
     ]
    }
   ],
   "source": [
    "#Make data for training \n",
    "import numpy as np\n",
    "#Use 50 categories\n",
    "index_seen = np.squeeze(np.where(label_seen < 201, True, False))\n",
    "index_unseen = np.squeeze(np.where(label_unseen < 201, True, False))\n",
    "\n",
    "#Use 20 categories\n",
    "#index_seen = np.squeeze(np.where(label_seen < 21, True, False))\n",
    "#index_unseen = np.squeeze(np.where(label_unseen < 21, True, False))\n",
    "\n",
    "brain_seen = brain_seen[index_seen, :]\n",
    "image_seen = image_seen[index_seen, :]\n",
    "text_seen = text_seen[index_seen, :]\n",
    "label_seen = label_seen[index_seen]\n",
    "brain_unseen = brain_unseen[index_unseen, :]\n",
    "image_unseen = image_unseen[index_unseen, :]\n",
    "text_unseen = text_unseen[index_unseen, :]\n",
    "label_unseen = label_unseen[index_unseen]\n",
    "\n",
    "#The ThingsEEG-Text dataset is mainly designed and used for Zero-Shot type research work, because the independence of its training set and test set\n",
    "#in categories is very suitable for this task. If it needs to be used for other types of tasks\n",
    "#(such as general classification or cross-modal learning),\n",
    "#the data may need to be repartitioned. Therefore, we repartition the dataset to make it better for our task\n",
    "#Define the number of classes and the number of samples per class\n",
    "num_classes = 50\n",
    "samples_per_class = 20\n",
    "train_ratio = 0.50 # 50% training and 50% testing\n",
    "#For each class, take the first 5 images as training and the last 5 images as testing\n",
    "new_train_brain = []\n",
    "new_train_image = []\n",
    "new_train_text = []\n",
    "new_train_label = []\n",
    "\n",
    "new_test_brain = []\n",
    "new_test_image = []\n",
    "new_test_text = []\n",
    "new_test_label = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    start_idx = i * samples_per_class#The starting index of the current class\n",
    "    end_idx = start_idx + samples_per_class#The end index of the current class\n",
    "    \n",
    "    #Get the data of the current class\n",
    "    class_data_brain = brain_seen[start_idx:end_idx, :]\n",
    "    class_data_image = image_seen[start_idx:end_idx, :]\n",
    "    class_data_text = text_seen[start_idx:end_idx, :]\n",
    "    class_data_label = label_seen[start_idx:end_idx, :]\n",
    "    \n",
    "    # Determine train and test split\n",
    "    num_train_samples = int(samples_per_class * train_ratio)\n",
    "    num_test_samples = samples_per_class - num_train_samples\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    new_train_brain.append(class_data_brain[:num_train_samples])\n",
    "    new_test_brain.append(class_data_brain[num_train_samples:])\n",
    "    \n",
    "    new_train_image.append(class_data_image[:num_train_samples])\n",
    "    new_test_image.append(class_data_image[num_train_samples:])\n",
    "    \n",
    "    new_train_text.append(class_data_text[:num_train_samples])\n",
    "    new_test_text.append(class_data_text[num_train_samples:])\n",
    "    \n",
    "    new_train_label.append(class_data_label[:num_train_samples])\n",
    "    new_test_label.append(class_data_label[num_train_samples:])\n",
    "\n",
    "\n",
    "train_brain = torch.vstack(new_train_brain)\n",
    "train_image = torch.vstack(new_train_image)\n",
    "train_text = torch.vstack(new_train_text)\n",
    "train_label = torch.vstack(new_train_label)\n",
    "test_brain = torch.vstack(new_test_brain)\n",
    "test_image = torch.vstack(new_test_image)\n",
    "test_text = torch.vstack(new_test_text)\n",
    "test_label = torch.vstack(new_test_label)\n",
    "\n",
    "# Check for NaN values in each tensor\n",
    "print(\"NaN values in train_brain:\", torch.sum(torch.isnan(train_brain)).item())\n",
    "print(\"NaN values in train_image:\", torch.sum(torch.isnan(train_image)).item())\n",
    "print(\"NaN values in train_text:\", torch.sum(torch.isnan(train_text)).item())\n",
    "print(\"NaN values in train_label:\", torch.sum(torch.isnan(train_label)).item())\n",
    "\n",
    "print(train_brain.shape)\n",
    "print(train_image.shape)\n",
    "print(train_text.shape)\n",
    "print(train_label.shape)\n",
    "print(test_brain.shape)\n",
    "print(test_image.shape)\n",
    "print(test_text.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3f937e-8d81-413c-9cdf-7d3f9631fb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in train_brain_np: 0\n",
      "NaN values in train_image_np: 0\n",
      "NaN values in train_text_np: 0\n",
      "NaN values in train_label_np: 0\n",
      "Class distribution in the training set:\n",
      "Class 1: 10 samples\n",
      "Class 3: 10 samples\n",
      "Class 5: 10 samples\n",
      "Class 7: 10 samples\n",
      "Class 9: 10 samples\n",
      "Class 11: 10 samples\n",
      "Class 13: 10 samples\n",
      "Class 15: 10 samples\n",
      "Class 17: 10 samples\n",
      "Class 19: 10 samples\n",
      "Class 21: 10 samples\n",
      "Class 23: 10 samples\n",
      "Class 25: 10 samples\n",
      "Class 27: 10 samples\n",
      "Class 29: 10 samples\n",
      "Class 31: 10 samples\n",
      "Class 33: 10 samples\n",
      "Class 35: 10 samples\n",
      "Class 37: 10 samples\n",
      "Class 39: 10 samples\n",
      "Class 41: 10 samples\n",
      "Class 43: 10 samples\n",
      "Class 45: 10 samples\n",
      "Class 47: 10 samples\n",
      "Class 49: 10 samples\n",
      "Class 51: 10 samples\n",
      "Class 53: 10 samples\n",
      "Class 55: 10 samples\n",
      "Class 57: 10 samples\n",
      "Class 59: 10 samples\n",
      "Class 61: 10 samples\n",
      "Class 63: 10 samples\n",
      "Class 65: 10 samples\n",
      "Class 67: 10 samples\n",
      "Class 69: 10 samples\n",
      "Class 71: 10 samples\n",
      "Class 73: 10 samples\n",
      "Class 75: 10 samples\n",
      "Class 77: 10 samples\n",
      "Class 79: 10 samples\n",
      "Class 81: 10 samples\n",
      "Class 83: 10 samples\n",
      "Class 85: 10 samples\n",
      "Class 87: 10 samples\n",
      "Class 89: 10 samples\n",
      "Class 91: 10 samples\n",
      "Class 93: 10 samples\n",
      "Class 95: 10 samples\n",
      "Class 97: 10 samples\n",
      "Class 99: 10 samples\n"
     ]
    }
   ],
   "source": [
    "#Converting tensor to numpy array \n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "train_brain_np = train_brain.numpy()\n",
    "train_image_np = train_image.numpy()\n",
    "train_text_np = train_text.numpy()\n",
    "train_label_np = train_label.numpy().ravel()\n",
    "\n",
    "test_brain_np = test_brain.numpy()\n",
    "test_image_np = test_image.numpy()\n",
    "test_text_np = test_text.numpy()\n",
    "test_label_np = test_label.numpy().ravel()\n",
    "\n",
    "#train_features = train_brain_np #we only use brain feature\n",
    "#test_features = test_brain_np\n",
    "\n",
    "# Check for NaN values in each array\n",
    "print(\"NaN values in train_brain_np:\", np.sum(np.isnan(train_brain_np)))\n",
    "print(\"NaN values in train_image_np:\", np.sum(np.isnan(train_image_np)))\n",
    "print(\"NaN values in train_text_np:\", np.sum(np.isnan(train_text_np)))\n",
    "print(\"NaN values in train_label_np:\", np.sum(np.isnan(train_label_np)))\n",
    "\n",
    "#Check class imbalance \n",
    "unique_classes, class_counts = np.unique(train_label_np, return_counts=True)\n",
    "print(\"Class distribution in the training set:\")\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    print(f\"Class {cls}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10b717e5-2bb4-4423-a015-109327278411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 10, 4: 10, 6: 10, 8: 10, 10: 10, 12: 10, 14: 10, 16: 10, 18: 10, 20: 10, 22: 10, 24: 10, 26: 10, 28: 10, 30: 10, 32: 10, 34: 10, 36: 10, 38: 10, 40: 10, 42: 10, 44: 10, 46: 10, 48: 10, 50: 10, 52: 10, 54: 10, 56: 10, 58: 10, 60: 10, 62: 10, 64: 10, 66: 10, 68: 10, 70: 10, 72: 10, 74: 10, 76: 10, 78: 10, 80: 10, 82: 10, 84: 10, 86: 10, 88: 10, 90: 10, 92: 10, 94: 10, 96: 10, 98: 10, 100: 10}\n"
     ]
    }
   ],
   "source": [
    "#Feature combination \n",
    "import numpy as np\n",
    "train_features_multiple = np.concatenate([train_brain_np, train_image_np, train_text_np], axis =1)\n",
    "test_features_multiple = np.concatenate([test_brain_np, test_image_np, test_text_np], axis =1)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Generate polynomial features, including interaction terms\n",
    "poly = PolynomialFeatures(degree=1, interaction_only=True, include_bias=False)\n",
    "train_features_multiple_interactions = poly.fit_transform(train_features_multiple)\n",
    "test_features_multiple_interactions = poly.fit_transform(test_features_multiple)\n",
    "\n",
    "unique, counts = np.unique(test_label_np, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "##From this point, no more libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b210a48-c479-4758-9c89-ef49b4d0f01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after oversampling: {1: 10, 3: 10, 5: 10, 7: 10, 9: 10, 11: 10, 13: 10, 15: 10, 17: 10, 19: 10, 21: 10, 23: 10, 25: 10, 27: 10, 29: 10, 31: 10, 33: 10, 35: 10, 37: 10, 39: 10, 41: 10, 43: 10, 45: 10, 47: 10, 49: 10, 51: 10, 53: 10, 55: 10, 57: 10, 59: 10, 61: 10, 63: 10, 65: 10, 67: 10, 69: 10, 71: 10, 73: 10, 75: 10, 77: 10, 79: 10, 81: 10, 83: 10, 85: 10, 87: 10, 89: 10, 91: 10, 93: 10, 95: 10, 97: 10, 99: 10}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time \n",
    "\n",
    "class CustomSVM:\n",
    "    def __init__(self, C=10.0, gamma='auto', max_iter=1000, tol=1e-3, class_weight=None):\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.class_weight = class_weight\n",
    "        self.classifiers = {}\n",
    "        self.classes = None\n",
    "        self.n_features = None\n",
    "        self.bias = 0.0  # Add bias as instance variable\n",
    "\n",
    "    def _compute_gamma(self, X):\n",
    "        \"\"\"Compute the gamma parameter if auto\"\"\"\n",
    "        if self.gamma == 'auto':\n",
    "            return 1.0 / (X.shape[1] * X.var()) if X.var() != 0 else 1.0\n",
    "        return self.gamma\n",
    "\n",
    "    def _compute_class_weights(self, y):\n",
    "        \"\"\"Compute class weights if needed\"\"\"\n",
    "        if self.class_weight == 'balanced':\n",
    "            classes = np.unique(y)\n",
    "            weights = {}\n",
    "            n_samples = len(y)\n",
    "            for c in classes:\n",
    "                weights[c] = n_samples / (len(classes) * np.sum(y == c))\n",
    "            return weights\n",
    "        return {c: 1.0 for c in np.unique(y)}\n",
    "\n",
    "    def rbf_kernel(self, x1, x2):\n",
    "        \"\"\"Compute RBF (Gaussian) kernel between two vectors\"\"\"\n",
    "        diff = x1 - x2\n",
    "        return np.exp(-self.gamma * np.sum(diff * diff))\n",
    "\n",
    "    def compute_kernel_matrix(self, X1, X2):\n",
    "        \"\"\"Compute the kernel matrix for given data points\"\"\"\n",
    "        n1, n2 = len(X1), len(X2)\n",
    "        K = np.zeros((n1, n2))\n",
    "        for i in range(n1):\n",
    "            for j in range(n2):\n",
    "                K[i,j] = self.rbf_kernel(X1[i], X2[j])\n",
    "        return K\n",
    "\n",
    "    def _train_one_vs_all(self, X, y, positive_class, class_weights):\n",
    "        \"\"\"Train a binary SVM for one class vs all others\"\"\"\n",
    "        n_samples = len(X)\n",
    "        alpha = np.zeros(n_samples)\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        # Convert to binary problem\n",
    "        binary_y = np.where(y == positive_class, 1, -1)\n",
    "        \n",
    "        # Compute kernel matrix\n",
    "        K = self.compute_kernel_matrix(X, X)\n",
    "        \n",
    "        # Compute sample weights\n",
    "        sample_weights = np.array([class_weights[y[i]] for i in range(n_samples)])\n",
    "        \n",
    "        # SMO algorithm with improvements\n",
    "        changed = True\n",
    "        iteration = 0\n",
    "        \n",
    "        while changed and iteration < self.max_iter:\n",
    "            changed = False\n",
    "            iteration += 1\n",
    "            \n",
    "            # First heuristic: examine all non-bound examples\n",
    "            for i in range(n_samples):\n",
    "                if 0 < alpha[i] < self.C * sample_weights[i]:\n",
    "                    if self._examine_example(i, alpha, binary_y, K, sample_weights):\n",
    "                        changed = True\n",
    "            \n",
    "            # Second heuristic: examine all examples\n",
    "            if not changed:\n",
    "                for i in range(n_samples):\n",
    "                    if self._examine_example(i, alpha, binary_y, K, sample_weights):\n",
    "                        changed = True\n",
    "        \n",
    "        # Save support vectors\n",
    "        support_vector_indices = alpha > 1e-5\n",
    "        return {\n",
    "            'alpha': alpha[support_vector_indices],\n",
    "            'support_vectors': X[support_vector_indices],\n",
    "            'support_vector_labels': binary_y[support_vector_indices],\n",
    "            'bias': self.bias\n",
    "        }\n",
    "\n",
    "    def _examine_example(self, i, alpha, y, K, sample_weights):\n",
    "        \"\"\"Examine one example for potential optimization\"\"\"\n",
    "        Ei = self._compute_error(i, alpha, y, K)\n",
    "        r = Ei * y[i]\n",
    "        \n",
    "        # Check if example violates KKT conditions\n",
    "        if (r < -self.tol and alpha[i] < self.C * sample_weights[i]) or \\\n",
    "           (r > self.tol and alpha[i] > 0):\n",
    "            \n",
    "            # Choose second example by maximum error difference\n",
    "            if len(alpha[alpha > 0]) > 1:\n",
    "                j = self._select_second_example(i, alpha, y, Ei, K)\n",
    "                if self._take_step(i, j, alpha, y, K, sample_weights):\n",
    "                    return True\n",
    "            \n",
    "            # Loop through all non-zero and non-C alpha, starting at random point\n",
    "            for j in np.random.permutation(len(alpha)):\n",
    "                if 0 < alpha[j] < self.C * sample_weights[j]:\n",
    "                    if self._take_step(i, j, alpha, y, K, sample_weights):\n",
    "                        return True\n",
    "            \n",
    "            # Loop through all examples, starting at random point\n",
    "            for j in np.random.permutation(len(alpha)):\n",
    "                if self._take_step(i, j, alpha, y, K, sample_weights):\n",
    "                    return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def _compute_error(self, i, alpha, y, K):\n",
    "        \"\"\"Compute error for example i\"\"\"\n",
    "        return sum(alpha[j] * y[j] * K[i,j] for j in range(len(y))) + self.bias - y[i]\n",
    "\n",
    "    def _select_second_example(self, i, alpha, y, Ei, K):\n",
    "        \"\"\"Select second example to maximize step size\"\"\"\n",
    "        max_delta_E = 0\n",
    "        j_max = i\n",
    "        valid_indices = np.where(alpha > 0)[0]\n",
    "        \n",
    "        for j in valid_indices:\n",
    "            if j == i:\n",
    "                continue\n",
    "            Ej = self._compute_error(j, alpha, y, K)\n",
    "            delta_E = abs(Ei - Ej)\n",
    "            if delta_E > max_delta_E:\n",
    "                max_delta_E = delta_E\n",
    "                j_max = j\n",
    "        \n",
    "        return j_max\n",
    "\n",
    "    def _take_step(self, i, j, alpha, y, K, sample_weights):\n",
    "        \"\"\"Attempt to optimize two examples\"\"\"\n",
    "        if i == j:\n",
    "            return False\n",
    "        \n",
    "        Ei = self._compute_error(i, alpha, y, K)\n",
    "        Ej = self._compute_error(j, alpha, y, K)\n",
    "        \n",
    "        alpha_i_old = alpha[i]\n",
    "        alpha_j_old = alpha[j]\n",
    "        \n",
    "        if y[i] != y[j]:\n",
    "            L = max(0, alpha_j_old - alpha_i_old)\n",
    "            H = min(self.C * sample_weights[j], \n",
    "                   self.C * sample_weights[j] + alpha_j_old - alpha_i_old)\n",
    "        else:\n",
    "            L = max(0, alpha_i_old + alpha_j_old - self.C * sample_weights[i])\n",
    "            H = min(self.C * sample_weights[j], alpha_i_old + alpha_j_old)\n",
    "        \n",
    "        if L == H:\n",
    "            return False\n",
    "        \n",
    "        eta = 2 * K[i,j] - K[i,i] - K[j,j]\n",
    "        if eta >= 0:\n",
    "            return False\n",
    "        \n",
    "        alpha[j] = alpha_j_old - y[j] * (Ei - Ej) / eta\n",
    "        alpha[j] = min(H, max(L, alpha[j]))\n",
    "        \n",
    "        if abs(alpha[j] - alpha_j_old) < 1e-5:\n",
    "            return False\n",
    "        \n",
    "        alpha[i] = alpha_i_old + y[i] * y[j] * (alpha_j_old - alpha[j])\n",
    "        \n",
    "        # Update bias\n",
    "        b1 = self.bias - Ei - y[i] * (alpha[i] - alpha_i_old) * K[i,i] \\\n",
    "             - y[j] * (alpha[j] - alpha_j_old) * K[i,j]\n",
    "        b2 = self.bias - Ej - y[i] * (alpha[i] - alpha_i_old) * K[i,j] \\\n",
    "             - y[j] * (alpha[j] - alpha_j_old) * K[j,j]\n",
    "        \n",
    "        if 0 < alpha[i] < self.C * sample_weights[i]:\n",
    "            self.bias = b1\n",
    "        elif 0 < alpha[j] < self.C * sample_weights[j]:\n",
    "            self.bias = b2\n",
    "        else:\n",
    "            self.bias = (b1 + b2) / 2\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train one SVM for each class\"\"\"\n",
    "        def oversample(X, y):\n",
    "            \"\"\"Manual oversampling to balance class distribution.\"\"\"\n",
    "            classes, class_counts = np.unique(y, return_counts=True)\n",
    "            max_class_count = np.max(class_counts)  # Majority class size\n",
    "\n",
    "            X_resampled = []\n",
    "            y_resampled = []\n",
    "\n",
    "            for cls, count in zip(classes, class_counts):\n",
    "                # Extract all samples belonging to the current class\n",
    "                X_class = X[y == cls]\n",
    "                y_class = y[y == cls]\n",
    "\n",
    "                # Oversample the class to match the majority class size\n",
    "                n_to_add = max_class_count - count\n",
    "                if n_to_add > 0:\n",
    "                    X_oversampled = np.vstack([X_class, X_class[np.random.choice(len(X_class), n_to_add, replace=True)]])\n",
    "                    y_oversampled = np.hstack([y_class, np.full(n_to_add, cls)])\n",
    "                else:\n",
    "                    X_oversampled = X_class\n",
    "                    y_oversampled = y_class\n",
    "\n",
    "                # Append to the resampled dataset\n",
    "                X_resampled.append(X_oversampled)\n",
    "                y_resampled.append(y_oversampled)\n",
    "\n",
    "            # Combine all classes\n",
    "            X_resampled = np.vstack(X_resampled)\n",
    "            y_resampled = np.hstack(y_resampled)\n",
    "\n",
    "            print(f\"Class distribution after oversampling: {dict(zip(*np.unique(y_resampled, return_counts=True)))}\")\n",
    "\n",
    "            return X_resampled, y_resampled\n",
    "\n",
    "        # Oversample the training data\n",
    "        X, y = oversample(X, y)\n",
    "\n",
    "        # Store oversampled data for debugging\n",
    "        self.X_oversampled = X\n",
    "        self.y_oversampled = y\n",
    "\n",
    "        self.n_features = X.shape[1]\n",
    "        self.gamma = self._compute_gamma(X)\n",
    "        self.classes = np.unique(y)\n",
    "        class_weights = self._compute_class_weights(y)\n",
    "    \n",
    "        # Scale features to [0, 1] range\n",
    "        X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0) + 1e-8)\n",
    "    \n",
    "        for class_label in self.classes:\n",
    "            self.classifiers[class_label] = self._train_one_vs_all(X, y, class_label, class_weights)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def _predict_one(self, x, classifier):\n",
    "        \"\"\"Get prediction score for one classifier\"\"\"\n",
    "        decision = 0\n",
    "        for alpha, sv_y, sv_x in zip(classifier['alpha'], \n",
    "                                   classifier['support_vector_labels'],\n",
    "                                   classifier['support_vectors']):\n",
    "            decision += alpha * sv_y * self.rbf_kernel(x, sv_x)\n",
    "        return decision + classifier['bias']\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels for samples in X\"\"\"\n",
    "        # Scale features using same scaling as in fit\n",
    "        X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0) + 1e-8)\n",
    "        \n",
    "        n_samples = len(X)\n",
    "        predictions = np.zeros((n_samples, len(self.classes)))\n",
    "        \n",
    "        # Get scores from all classifiers\n",
    "        for i, class_label in enumerate(self.classes):\n",
    "            classifier = self.classifiers[class_label]\n",
    "            for j, x in enumerate(X):\n",
    "                predictions[j, i] = self._predict_one(x, classifier)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        exp_preds = np.exp(predictions - np.max(predictions, axis=1, keepdims=True))\n",
    "        probabilities = exp_preds / np.sum(exp_preds, axis=1, keepdims=True)\n",
    "        \n",
    "        # Return class with highest probability\n",
    "        return self.classes[np.argmax(probabilities, axis=1)]\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"Compute accuracy score\"\"\"\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def classification_report(y_true, y_pred):\n",
    "    \"\"\"Generate a classification report\"\"\"\n",
    "    classes = np.unique(y_true)\n",
    "    report = \"\"\n",
    "    \n",
    "    # Compute macro averages\n",
    "    macro_precision = 0\n",
    "    macro_recall = 0\n",
    "    macro_f1 = 0\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    for c in classes:\n",
    "        true_pos = np.sum((y_true == c) & (y_pred == c))\n",
    "        false_pos = np.sum((y_true != c) & (y_pred == c))\n",
    "        false_neg = np.sum((y_true == c) & (y_pred != c))\n",
    "        \n",
    "        precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "        recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        macro_precision += precision\n",
    "        macro_recall += recall\n",
    "        macro_f1 += f1\n",
    "        \n",
    "        report += f\"Class {c}:\\n\"\n",
    "        report += f\"Precision: {precision:.3f}\\n\"\n",
    "        report += f\"Recall: {recall:.3f}\\n\"\n",
    "        report += f\"F1-score: {f1:.3f}\\n\\n\"\n",
    "    \n",
    "    # Add macro averages to report\n",
    "    report += \"Macro Averages:\\n\"\n",
    "    report += f\"Precision: {macro_precision/n_classes:.3f}\\n\"\n",
    "    report += f\"Recall: {macro_recall/n_classes:.3f}\\n\"\n",
    "    report += f\"F1-score: {macro_f1/n_classes:.3f}\\n\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Initializing the CustomSVM\n",
    "svm = CustomSVM(C=10.0, gamma='auto', max_iter=1000, class_weight='balanced')\n",
    "\n",
    "# Train\n",
    "start_train_time = time.time()\n",
    "svm.fit(train_features_multiple_interactions, train_label_np)\n",
    "end_train_time = time.time()\n",
    "\n",
    "# Predict\n",
    "start_test_time = time.time()\n",
    "predictions = svm.predict(test_features_multiple_interactions)\n",
    "end_test_time = time.time()\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(test_label_np, predictions))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_label_np, predictions))\n",
    "print(f\"Training time: {end_train_time - start_train_time:.2f} seconds\")\n",
    "print(f\"Training time: {end_train_time - start_train_time:.2f} seconds\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_class_distribution(X, y, title=\"Class Distribution\"):\n",
    "    \"\"\"Plot class distribution.\"\"\"\n",
    "    classes = np.unique(y)\n",
    "    for cls in classes:\n",
    "        plt.scatter(X[y == cls, 0], X[y == cls, 1], label=f\"Class {cls}\", alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Before oversampling\n",
    "plot_class_distribution(train_features_multiple, train_label_np, title=\"Before Oversampling\")\n",
    "\n",
    "# After oversampling\n",
    "plot_class_distribution(svm.X_oversampled, svm.y_oversampled, title=\"After Oversampling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b505878-291b-4b4d-a1a3-da3082e55b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
